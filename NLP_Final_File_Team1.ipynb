{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "from scipy import spatial\n",
    "import networkx as nx\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import bs4 as BeautifulSoup\n",
    "import urllib.request  \n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetching the content from the URL\n",
    "fetched_data = urllib.request.urlopen('https://en.wikipedia.org/wiki/20th_century')\n",
    "article_read = fetched_data.read()\n",
    "\n",
    "#parsing the URL content and storing in a variable\n",
    "article_parsed = BeautifulSoup.BeautifulSoup(article_read,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returning <p> tags\n",
    "paragraphs = article_parsed.find_all('p')\n",
    "\n",
    "article_content = ''\n",
    "\n",
    "#looping through the paragraphs and adding them to the variable\n",
    "for p in paragraphs:  \n",
    "    article_content += p.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_dictionary_table(text_string) -> dict:\n",
    "   \n",
    "    #removing stop words\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    words = word_tokenize(text_string)\n",
    "    \n",
    "    #reducing words to their root form\n",
    "    stem = PorterStemmer()\n",
    "    \n",
    "    #creating dictionary for the word frequency table\n",
    "    frequency_table = dict()\n",
    "    for wd in words:\n",
    "        wd = stem.stem(wd)\n",
    "        if wd in stop_words:\n",
    "            continue\n",
    "        if wd in frequency_table:\n",
    "            frequency_table[wd] += 1\n",
    "        else:\n",
    "            frequency_table[wd] = 1\n",
    "       \n",
    "    return frequency_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calculate_sentence_scores(sentences, frequency_table) -> dict:   \n",
    "\n",
    "    #algorithm for scoring a sentence by its words\n",
    "    sentence_weight = dict()\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence_wordcount = (len(word_tokenize(sentence)))\n",
    "        sentence_wordcount_without_stop_words = 0\n",
    "        for word_weight in frequency_table:\n",
    "            if word_weight in sentence.lower():\n",
    "                sentence_wordcount_without_stop_words += 1\n",
    "                if sentence[:7] in sentence_weight:\n",
    "                    sentence_weight[sentence[:7]] += frequency_table[word_weight]\n",
    "                else:\n",
    "                    sentence_weight[sentence[:7]] = frequency_table[word_weight]\n",
    "\n",
    "        sentence_weight[sentence[:7]] = sentence_weight[sentence[:7]] / sentence_wordcount_without_stop_words\n",
    "\n",
    "       \n",
    "\n",
    "    return sentence_weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calculate_average_score(sentence_weight) -> int:\n",
    "   \n",
    "    #calculating the average score for the sentences\n",
    "    sum_values = 0\n",
    "    for entry in sentence_weight:\n",
    "        sum_values += sentence_weight[entry]\n",
    "\n",
    "    #getting sentence average value from source text\n",
    "    average_score = (sum_values / len(sentence_weight))\n",
    "\n",
    "    return average_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_article_summary(sentences, sentence_weight, threshold):\n",
    "    sentence_counter = 0\n",
    "    article_summary = ''\n",
    "\n",
    "    for sentence in sentences:\n",
    "        if sentence[:7] in sentence_weight and sentence_weight[sentence[:7]] >= (threshold):\n",
    "            article_summary += \" \" + sentence\n",
    "            sentence_counter += 1\n",
    "\n",
    "    return article_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.cluster.util import cosine_distance\n",
    "\n",
    "MULTIPLE_WHITESPACE_PATTERN = re.compile(r\"\\s+\", re.UNICODE)\n",
    "\n",
    "\n",
    "def normalize_whitespace(text):\n",
    "    \"\"\"\n",
    "    Translates multiple whitespace into single space character.\n",
    "    If there is at least one new line character chunk is replaced\n",
    "    by single LF (Unix new line) character.\n",
    "    \"\"\"\n",
    "    return MULTIPLE_WHITESPACE_PATTERN.sub(_replace_whitespace, text)\n",
    "\n",
    "\n",
    "def _replace_whitespace(match):\n",
    "    text = match.group()\n",
    "\n",
    "    if \"\\n\" in text or \"\\r\" in text:\n",
    "        return \"\\n\"\n",
    "    else:\n",
    "        return \" \"\n",
    "\n",
    "\n",
    "def is_blank(string):\n",
    "    \"\"\"\n",
    "    Returns `True` if string contains only white-space characters\n",
    "    or is empty. Otherwise `False` is returned.\n",
    "    \"\"\"\n",
    "    return not string or string.isspace()\n",
    "\n",
    "\n",
    "def get_symmetric_matrix(matrix):\n",
    "    \"\"\"\n",
    "    Get Symmetric matrix\n",
    "    :param matrix:\n",
    "    :return: matrix\n",
    "    \"\"\"\n",
    "    return matrix + matrix.T - np.diag(matrix.diagonal())\n",
    "\n",
    "\n",
    "def core_cosine_similarity(vector1, vector2):\n",
    "    \"\"\"\n",
    "    measure cosine similarity between two vectors\n",
    "    :param vector1:\n",
    "    :param vector2:\n",
    "    :return: 0 < cosine similarity value < 1\n",
    "    \"\"\"\n",
    "    return 1 - cosine_distance(vector1, vector2)\n",
    "\n",
    "\n",
    "'''\n",
    "Note: This is not a summarization algorithm. This Algorithm pics top sentences irrespective of the order they appeared.\n",
    "'''\n",
    "\n",
    "\n",
    "class TextRank4Sentences():\n",
    "    def __init__(self):\n",
    "        self.damping = 0.85  # damping coefficient, usually is .85\n",
    "        self.min_diff = 1e-5  # convergence threshold\n",
    "        self.steps = 100  # iteration steps\n",
    "        self.text_str = None\n",
    "        self.sentences = None\n",
    "        self.pr_vector = None\n",
    "\n",
    "    def _sentence_similarity(self, sent1, sent2, stopwords=None):\n",
    "        if stopwords is None:\n",
    "            stopwords = []\n",
    "\n",
    "        sent1 = [w.lower() for w in sent1]\n",
    "        sent2 = [w.lower() for w in sent2]\n",
    "\n",
    "        all_words = list(set(sent1 + sent2))\n",
    "\n",
    "        vector1 = [0] * len(all_words)\n",
    "        vector2 = [0] * len(all_words)\n",
    "\n",
    "        # build the vector for the first sentence\n",
    "        for w in sent1:\n",
    "            if w in stopwords:\n",
    "                continue\n",
    "            vector1[all_words.index(w)] += 1\n",
    "\n",
    "        # build the vector for the second sentence\n",
    "        for w in sent2:\n",
    "            if w in stopwords:\n",
    "                continue\n",
    "            vector2[all_words.index(w)] += 1\n",
    "\n",
    "        return core_cosine_similarity(vector1, vector2)\n",
    "\n",
    "    def _build_similarity_matrix(self, sentences, stopwords=None):\n",
    "        # create an empty similarity matrix\n",
    "        sm = np.zeros([len(sentences), len(sentences)])\n",
    "\n",
    "        for idx1 in range(len(sentences)):\n",
    "            for idx2 in range(len(sentences)):\n",
    "                if idx1 == idx2:\n",
    "                    continue\n",
    "\n",
    "                sm[idx1][idx2] = self._sentence_similarity(sentences[idx1], sentences[idx2], stopwords=stopwords)\n",
    "\n",
    "        # Get Symmeric matrix\n",
    "        sm = get_symmetric_matrix(sm)\n",
    "\n",
    "        # Normalize matrix by column\n",
    "        norm = np.sum(sm, axis=0)\n",
    "        sm_norm = np.divide(sm, norm, where=norm != 0)  # this is ignore the 0 element in norm\n",
    "\n",
    "        return sm_norm\n",
    "\n",
    "    def _run_page_rank(self, similarity_matrix):\n",
    "\n",
    "        pr_vector = np.array([1] * len(similarity_matrix))\n",
    "\n",
    "        # Iteration\n",
    "        previous_pr = 0\n",
    "        for epoch in range(self.steps):\n",
    "            pr_vector = (1 - self.damping) + self.damping * np.matmul(similarity_matrix, pr_vector)\n",
    "            if abs(previous_pr - sum(pr_vector)) < self.min_diff:\n",
    "                break\n",
    "            else:\n",
    "                previous_pr = sum(pr_vector)\n",
    "\n",
    "        return pr_vector\n",
    "\n",
    "    def _get_sentence(self, index):\n",
    "\n",
    "        try:\n",
    "            return self.sentences[index]\n",
    "        except IndexError:\n",
    "            return \"\"\n",
    "\n",
    "    def get_top_sentences(self, number=5):\n",
    "\n",
    "        top_sentences = []\n",
    "\n",
    "        if self.pr_vector is not None:\n",
    "\n",
    "            sorted_pr = np.argsort(self.pr_vector)\n",
    "            sorted_pr = list(sorted_pr)\n",
    "            sorted_pr.reverse()\n",
    "\n",
    "            index = 0\n",
    "            for epoch in range(number):\n",
    "                sent = self.sentences[sorted_pr[index]]\n",
    "                sent = normalize_whitespace(sent)\n",
    "                top_sentences.append(sent)\n",
    "                index += 1\n",
    "\n",
    "        return top_sentences\n",
    "\n",
    "    def analyze(self, text, stop_words=None):\n",
    "        self.text_str = text\n",
    "        self.sentences = sent_tokenize(self.text_str)\n",
    "\n",
    "        tokenized_sentences = [word_tokenize(sent) for sent in self.sentences]\n",
    "\n",
    "        similarity_matrix = self._build_similarity_matrix(tokenized_sentences, stop_words)\n",
    "\n",
    "        self.pr_vector = self._run_page_rank(similarity_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _run_article_summary(article):\n",
    "    \n",
    "    #creating a dictionary for the word frequency table\n",
    "    frequency_table = _create_dictionary_table(article)\n",
    " \n",
    "    #tokenizing the sentences\n",
    "    sentences = sent_tokenize(article)\n",
    "    \n",
    "    #algorithm for scoring a sentence by its words\n",
    "    sentence_scores = _calculate_sentence_scores(sentences, frequency_table)\n",
    "\n",
    "    #getting the threshold\n",
    "    threshold = _calculate_average_score(sentence_scores)\n",
    "\n",
    "    #producing the summary\n",
    "    article_summary = _get_article_summary(sentences, sentence_scores, 1.5 * threshold)\n",
    "\n",
    "    return article_summary,frequency_table,sentence_scores,threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Humans explored space for the first time, taking their first footsteps on the Moon. However, these same wars resulted in the destruction of the imperial system. The victorious Bolsheviks then established the Soviet Union, the world's first communist state. At the beginning of the period, the British Empire was the world's most powerful nation,[14] having acted as the world's policeman for the past century. In total, World War II left some 60 million people dead. With the Axis defeated and Britain and France rebuilding, the United States and the Soviet Union were left standing as the world's only superpowers. At the beginning of the century, strong discrimination based on race and sex was significant in general society. During the century, the social taboo of sexism fell. With the end of colonialism and the Cold War, nearly a billion people in Africa were left in new nation states after centuries of foreign domination. The world was undergoing its second major period of globalization; the first, which started in the 18th century, having been terminated by World War I. Since the US was in a dominant position, a major part of the process was Americanization. Terrorism, dictatorship, and the spread of nuclear weapons were pressing global issues. The world was still blighted by small-scale wars and other violent conflicts, fueled by competition over resources and by ethnic conflicts. Millions were infected with HIV, the virus which causes AIDS. This includes deaths caused by wars, genocide, politicide and mass murders. Later in the 20th century, the development of computers led to the establishment of a theory of computation.\n",
      " After the victory of the Allies in Europe, the war in Asia ended with the Soviet invasion of Manchuria and the dropping of two atomic bombs on Japan by the US, the first nation to develop nuclear weapons and the only one to use them in warfare. The people of the Indian subcontinent, a sixth of the world population at the end of the 20th century, had attained an indigenous independence for the first time in centuries. Fascism, a movement which grew out of post-war angst and which accelerated during the Great Depression of the 1930s, gained momentum in Italy, Germany, and Spain in the 1920s and 1930s, culminating in World War II, sparked by Nazi Germany's aggressive expansion at the expense of its neighbors. Scientific discoveries, such as the theory of relativity and quantum physics, profoundly changed the foundational models of physical science, forcing scientists to realize that the universe was more complex than previously believed, and dashing the hopes (or fears) at the end of the 19th century that the last few details of scientific knowledge were about to be filled in. After some years of dramatic military success, Germany was defeated in 1945, having been invaded by the Soviet Union and Poland from the East and by the United States, the United Kingdom, Canada, and France from the West.\n"
     ]
    }
   ],
   "source": [
    "if __name__== '__main__':\n",
    "    summary_results,freq_table,sent_score,avg_score = _run_article_summary(article_content)\n",
    "    print(summary_results)\n",
    "    trial = TextRank4Sentences()\n",
    "    trial.analyze(article_content)\n",
    "    topn=trial.get_top_sentences(5)\n",
    "    summary_topn=''\n",
    "    for sentence in topn:\n",
    "            summary_topn += \" \" + sentence\n",
    "    print(summary_topn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'20th': 21, '(': 12, 'twentieth': 2, ')': 12, 'centuri': 47, 'began': 4, 'januari': 3, '1': 6, ',': 306, '1901': 3, 'mcmi': 1, 'end': 15, 'decemb': 3, '31': 3, '2000': 3, 'mm': 1, '.': 126, '[': 39, ']': 39, 'wa': 35, 'domin': 3, 'signific': 4, 'event': 1, 'defin': 1, 'era': 1, ':': 4, 'spanish': 1, 'flu': 1, 'pandem': 1, 'world': 48, 'war': 39, 'ii': 8, 'nuclear': 10, 'weapon': 6, 'power': 12, 'space': 4, 'explor': 3, 'nation': 18, 'decolon': 3, 'technolog': 21, 'advanc': 8, 'cold': 5, 'post-cold': 1, 'conflict': 8, 'saw': 2, 'massiv': 2, 'transform': 2, 'order': 2, 'global': 17, 'total': 3, 'fertil': 1, 'rate': 1, 'sea': 1, 'level': 2, 'rise': 3, 'ecolog': 2, 'collaps': 3, 'increas': 3, ';': 9, 'result': 10, 'competit': 3, 'land': 1, 'dwindl': 1, 'resourc': 3, 'acceler': 2, 'deforest': 1, 'water': 1, 'deplet': 1, 'mass': 3, 'extinct': 2, 'mani': 14, \"'s\": 20, 'speci': 1, 'declin': 2, 'popul': 11, 'consequ': 1, 'dealt': 1, 'man-mad': 1, 'warm': 2, 'extrem': 1, 'weather': 1, 'condit': 1, 'addit': 2, 'theme': 1, 'includ': 5, 'intergovernment': 1, 'organ': 4, 'cultur': 7, 'homogen': 2, 'develop': 16, 'emerg': 3, 'transport': 4, 'commun': 10, 'poverti': 2, 'reduct': 1, 'growth': 2, 'awar': 1, 'environment': 4, 'degrad': 1, '2': 2, '3': 1, 'birth': 2, 'digit': 2, 'revolut': 9, 'automobil': 4, 'airplan': 1, 'use': 4, 'home': 1, 'applianc': 1, 'becam': 8, 'common': 2, 'video': 2, 'audio': 1, 'record': 4, 'great': 3, 'gener': 3, 'medic': 4, 'allow': 3, 'near-instantan': 1, 'worldwid': 1, 'comput': 6, 'genet': 1, 'modif': 1, 'life': 6, 'repercuss': 1, 'craft': 1, 'peopl': 14, 'unit': 13, 'ani': 1, 'previou': 1, 'time': 9, 'human': 9, 'histori': 6, 'exemplifi': 1, 'establish': 8, 'intern': 8, 'law': 2, 'aid': 4, 'marshal': 2, 'plan—which': 1, 'spent': 1, '$': 2, '13': 1, 'billion': 8, '100': 2, '2019': 1, 'us': 6, 'dollar': 1, '4': 1, 'rebuild': 2, 'economi': 4, 'post-war': 4, 'nations—launch': 1, '``': 5, 'pax': 1, 'americana': 1, \"''\": 3, 'throughout': 1, 'latter': 2, 'half': 4, 'rivalri': 1, 'state': 11, 'soviet': 9, 'union': 8, 'creat': 3, 'enorm': 1, 'tension': 1, 'around': 4, 'manifest': 1, 'variou': 3, 'arm': 4, 'proxi': 2, 'region': 3, 'omnipres': 1, 'danger': 1, 'prolifer': 2, 'dissolut': 1, '1991': 2, 'european': 4, 'allianc': 2, 'herald': 1, 'west': 3, 'though': 1, 'roughli': 1, 'one': 7, 'six': 1, 'earth': 1, 'live': 4, 'communist': 4, 'rule': 2, 'mostli': 1, 'china': 5, 'rapidli': 3, 'econom': 6, 'geopolit': 1, 'took': 1, 'two-hundr': 1, 'thousand': 2, 'year': 13, 'modern': 2, '6': 3, 'million': 9, 'evolut': 1, '1804': 1, 'reach': 5, '5': 1, 'estim': 4, '1927': 2, 'late': 3, '1999': 2, 'concentr': 1, 'east': 4, 'south': 2, 'southeast': 1, 'asia': 4, '7': 1, '8': 1, 'literaci': 1, 'averag': 1, '80': 2, '%': 1, 'penicillin': 1, 'breakthrough': 1, 'combin': 3, 'health': 2, 'campaign': 1, 'erad': 1, 'smallpox': 2, 'diseas': 5, 'respons': 1, 'death': 4, 'natur': 2, 'disast': 1, 'yield': 1, 'unpreced': 3, 'onli': 4, 'exist': 1, 'lab': 1, '9': 1, 'machin': 1, 'util': 1, 'area': 1, 'product': 2, 'feed': 1, 'increasingli': 1, 'intric': 1, 'suppli': 1, 'chain': 1, 'mean': 2, 'first': 18, 'mankind': 1, 'longer': 2, 'constrain': 1, 'much': 1, 'could': 2, 'produc': 4, 'rather': 1, \"'\": 1, 'willing': 1, 'consum': 1, 'trade': 3, 'improv': 2, 'revers': 1, 'limit': 2, 'set': 3, 'food-produc': 1, 'techniqu': 1, 'sinc': 5, 'neolith': 1, 'period': 4, 'greatli': 1, 'enhanc': 1, 'divers': 1, 'food': 3, 'avail': 2, 'upturn': 1, 'qualiti': 2, 'nutrit': 1, 'earli': 1, '19th': 4, 'expect': 2, 'thirti': 1, 'lifespan-averag': 1, 'exceed': 1, '40': 2, 'achiev': 1, '70': 2, 'three': 1, 'decad': 2, 'earlier': 1, '10': 1, '11': 1, 'term': 3, 'often': 1, 'erron': 1, 'refer': 1, '1900': 2, 'tenth': 1, 'final': 2, '2nd': 1, 'millennium': 1, 'unlik': 1, 'leap': 2, 'second': 4, 'gregorian': 1, 'calendar': 1, '1600': 1, 'global-scal': 1, 'across': 2, 'contin': 1, 'ocean': 2, 'major': 7, 'polit': 4, 'issu': 3, 'acknowledg': 1, 'along': 2, 'right': 3, 'self-determin': 1, 'offici': 1, 'mid-centuri': 1, 'relat': 1, 'shift': 1, 'way': 3, 'chang': 6, 'ideolog': 3, 'societi': 4, 'scienc': 3, 'medicin': 2, 'may': 2, 'seen': 3, 'scientif': 5, 'progress': 1, 'dawn': 1, 'civil': 2, 'like': 2, 'genocid': 2, 'enter': 1, 'usag': 1, 'discoveri': 1, 'theori': 4, 'rel': 1, 'quantum': 1, 'physic': 2, 'profoundli': 1, 'foundat': 1, 'model': 1, 'forc': 2, 'scientist': 3, 'realiz': 1, 'univers': 1, 'complex': 1, 'previous': 1, 'believ': 1, 'dash': 1, 'hope': 1, 'fear': 1, 'last': 2, 'detail': 1, 'knowledg': 2, 'fill': 1, 'start': 2, 'hors': 2, 'simpl': 1, 'freighter': 1, 'high-spe': 1, 'rail': 1, 'cruis': 1, 'ship': 1, 'commerci': 1, 'air': 1, 'travel': 2, 'shuttl': 1, 'pack': 1, 'anim': 3, 'everi': 1, 'basic': 1, 'form': 7, 'person': 1, 'replac': 2, 'buse': 1, 'within': 1, 'made': 3, 'possibl': 1, 'exploit': 1, 'fossil': 2, 'fuel': 3, 'offer': 1, 'energi': 1, 'easili': 1, 'portabl': 1, 'also': 8, 'caus': 4, 'concern': 1, 'pollut': 1, 'long-term': 1, 'impact': 1, 'environ': 1, 'take': 1, 'footstep': 1, 'moon': 1, 'media': 1, 'telecommun': 1, 'inform': 4, 'especi': 1, 'paperback': 1, 'book': 1, 'public': 1, 'educ': 1, 'internet': 3, 'wide': 1, '35': 1, '65': 1, 'rapid': 1, 'howev': 2, 'warfar': 4, 'destruct': 4, 'alon': 2, 'kill': 5, '60': 2, 'gave': 1, 'humankind': 1, 'annihil': 1, 'short': 1, 'imperi': 1, 'system': 1, 'empir': 4, 'expans': 2, 'colon': 1, 'ceas': 1, 'factor': 1, 'affair': 2, 'far': 2, 'cooper': 1, 'clash': 1, 'openli': 1, '1945': 3, 'violenc': 2, 'ha': 1, '12': 1, 'ever': 2, 'popular': 2, 'music': 8, 'influenc': 4, 'western': 4, 'corpor': 1, 'arguabl': 1, 'truli': 1, 'dure': 7, 'fought': 1, 'new': 8, 'invent': 3, 'tank': 1, 'chemic': 1, 'aircraft': 1, 'modifi': 1, 'tactic': 1, 'strategi': 1, 'four': 2, 'trench': 1, 'europ': 8, '20': 2, 'dead': 2, 'tripl': 2, 'entent': 2, 'franc': 3, 'britain': 3, 'russia': 1, 'later': 3, 'join': 1, 'itali': 3, 'romania': 1, 'victori': 3, 'central': 1, 'germani': 8, 'austria-hungari': 1, 'ottoman': 2, 'bulgaria': 1, 'annex': 1, 'coloni': 2, 'possess': 1, 'vanquish': 1, 'exact': 1, 'punit': 1, 'restitut': 1, 'payment': 1, 'plung': 1, 'particular': 1, 'depress': 2, 'austro-hungarian': 1, 'dismantl': 1, 'conclus': 1, 'russian': 2, 'overthrow': 1, 'tsarist': 1, 'regim': 1, 'nichola': 1, 'onset': 1, 'bolshevik': 1, 'begin': 2, 'british': 2, '14': 1, 'act': 2, 'policeman': 1, 'past': 1, 'fascism': 1, 'movement': 1, 'grew': 1, 'angst': 1, '1930': 2, 'gain': 2, 'momentum': 1, 'spain': 1, '1920': 3, 'culmin': 2, 'spark': 1, 'nazi': 1, 'aggress': 1, 'expens': 1, 'neighbor': 1, 'meanwhil': 2, 'japan': 3, 'industri': 4, 'axi': 2, 'militari': 5, 'expansion': 1, 'pacif': 1, 'brought': 1, 'surpris': 1, 'attack': 1, 'drew': 1, 'dramat': 2, 'success': 1, 'defeat': 2, 'invad': 1, 'poland': 1, 'kingdom': 1, 'canada': 1, 'alli': 3, 'invas': 2, 'manchuria': 1, 'drop': 1, 'two': 3, 'atom': 1, 'bomb': 1, 'left': 3, 'occupi': 1, 'divid': 2, 'rest': 1, 'eastern': 1, 'puppet': 1, 'rebuilt': 1, 'american': 5, 'plan': 1, 'boom': 1, 'affect': 3, 'close': 1, 'stand': 1, 'superpow': 1, 'soon': 1, 'hostil': 1, 'anoth': 3, 'compet': 2, 'democrat': 1, 'capit': 1, 'iron': 1, 'curtain': 1, 'berlin': 1, 'wall': 1, 'nato': 1, 'warsaw': 1, 'pact': 1, 'engag': 2, 'decades-long': 1, 'standoff': 1, 'known': 1, 'mark': 1, 'race': 2, 'ussr': 2, 'side': 3, 'suffici': 1, 'number': 3, 'planet': 2, 'large-scal': 1, 'exchang': 2, 'occur': 2, 'mutual': 1, 'assur': 1, 'credit': 1, 'historian': 1, 'prevent': 1, 'unabl': 2, 'strike': 2, 'without': 1, 'ensur': 1, 'equal': 2, 'devast': 1, 'retaliatori': 1, 'directli': 2, 'play': 1, 'seri': 1, 'world—particularli': 1, 'korea': 2, 'cuba': 2, 'vietnam': 2, 'afghanistan—a': 1, 'sought': 1, 'export': 1, 'attempt': 1, 'contain': 1, 'led': 5, 'substanti': 1, 'invest': 1, 'research': 4, 'innov': 1, 'beyond': 1, 'battlefield': 1, 'european-colon': 1, 'africa': 4, 'independ': 2, 'process': 2, 'open': 2, 'door': 1, 'sever': 1, 'exert': 1, 'strong': 2, 'presenc': 1, 'spread': 3, 'advent': 1, 'hollywood': 2, 'motion': 1, 'pictur': 1, 'broadway': 1, 'rock': 2, 'roll': 2, 'pop': 1, 'fast': 1, 'big-box': 1, 'store': 1, 'hip-hop': 1, 'lifestyl': 1, 'continu': 4, 'lead': 1, 'band': 1, 'countri': 3, 'swedish': 1, 'abba': 2, 'sing': 1, 'english': 1, 'pressur': 1, 'govern': 3, 'support': 1, 'dismantled—with': 1, 'notabl': 1, 'except': 1, 'north': 2, 'laos—follow': 1, 'awkward': 1, 'transit': 1, 'market': 1, 'follow': 2, 'successor': 1, 'leagu': 1, 'forum': 1, 'discuss': 1, 'diplomat': 1, 'enact': 1, 'resolut': 1, 'topic': 1, 'conduct': 1, 'protect': 1, 'sovereignti': 1, 'peacekeep': 1, 'consist': 2, 'troop': 1, 'provid': 1, 'agenc': 1, 'help': 1, 'reliev': 1, 'famin': 2, 'suppress': 1, 'local': 1, 'slowli': 1, '15': 2, 'due': 2, 'expand': 2, 'indirectli': 1, 'light': 1, 'bulb': 1, 'telephon': 2, 'supertank': 1, 'airlin': 1, 'motorway': 1, 'radio': 2, 'televis': 1, 'antibiot': 1, 'frozen': 1, 'microcomput': 1, 'mobil': 1, 'engin': 2, 'profession': 1, 'development—much': 1, 'motiv': 1, 'race—drov': 1, 'everyday': 1, 'discrimin': 1, 'base': 2, 'sex': 1, 'although': 1, 'atlant': 1, 'slave': 1, 'fight': 1, 'non-whit': 1, 'white-domin': 1, 'america': 1, 'social': 1, 'taboo': 1, 'sexism': 1, 'fell': 1, 'women': 1, 'legal': 1, 'men': 1, 'part': 4, 'racism': 1, 'come': 1, 'abhorr': 1, 'attitud': 1, 'toward': 1, 'homosexu': 1, 'radic': 1, 'alter': 1, 'daili': 1, 'appear': 1, 'sustain': 1, 'peac': 1, 'indian': 1, 'subcontin': 1, 'sixth': 1, 'attain': 1, 'indigen': 1, 'ancient': 1, 'compris': 1, 'fifth': 1, 'near-complet': 1, 'old': 1, 'nearli': 1, 'foreign': 1, 'undergo': 1, '18th': 1, 'termin': 1, 'posit': 1, 'india': 1, 'largest': 1, 'integr': 1, 'terror': 1, 'dictatorship': 1, 'press': 1, 'still': 1, 'blight': 1, 'small-scal': 1, 'violent': 1, 'ethnic': 1, 'threaten': 2, 'destabil': 1, 'virus': 1, 'nile': 1, 'viru': 3, 'malaria': 1, 'larg': 1, 'infect': 1, 'hiv': 1, 'becom': 1, 'epidem': 1, 'southern': 1, 'done': 1, 'climat': 1, 'consid': 1, 'long': 1, 'problem': 1, 'habit': 1, '16': 1, 'argument': 1, 'human-caus': 1, 'emiss': 2, 'greenhous': 1, 'gase': 1, 'particularli': 1, 'carbon': 2, 'dioxid': 2, 'burn': 1, '17': 1, 'thi': 2, 'prompt': 1, 'negoti': 1, 'sign': 1, 'kyoto': 1, 'treati': 1, 'mandatori': 1, '1.6': 1, '6.1': 1, '18': 1, '19': 1, 'action': 2, 'hundr': 1, 'politicid': 1, 'murder': 1, '50': 1, 'citat': 1, 'need': 1, 'rudolph': 1, 'rummel': 1, '262,000,000': 1, 'democid': 1, 'exclud': 1, 'battl': 1, 'civilian': 2, 'unintent': 1, 'riot': 1, 'mob': 1, 'accord': 1, 'charl': 1, 'tilli': 1, 'altogeth': 1, 'die': 3, 'direct': 1, 'back': 1, 'cours': 1, 'compar': 1, 'war-induc': 1, 'indirect': 1, 'effect': 1, '21': 1, 'approxim': 1, '1914': 1, '22': 1, 'phonograph': 1, 'dissemin': 1, 'broadcast': 1, 'audienc': 1, 'prior': 1, 'experienc': 1, 'perform': 1, 'genr': 1, 'artist': 2, 'béla': 1, 'bartók': 1, 'carl': 1, 'orff': 1, 'benjamin': 1, 'britten': 1, 'aaron': 1, 'copland': 1, 'georg': 1, 'gershwin': 1, 'ernesto': 1, 'lecuona': 1, 'gustav': 1, 'mahler': 1, 'sergei': 2, 'prokofiev': 1, 'rachmaninoff': 1, 'leonard': 2, 'bernstein': 1, 'mauric': 1, 'ravel': 1, 'arnold': 1, 'schoenberg': 1, 'dmitri': 1, 'shostakovich': 1, 'igor': 1, 'stravinski': 1, 'philip': 1, 'glass': 1, 'richard': 1, 'strauss': 1, 'beach': 1, 'boy': 1, 'beatl': 1, 'harri': 1, 'belafont': 1, 'chuck': 1, 'berri': 1, 'jame': 1, 'brown': 1, 'cher': 1, 'cohen': 1, 'bob': 2, 'dylan': 1, 'aretha': 1, 'franklin': 1, 'jimi': 1, 'hendrix': 1, 'michael': 1, 'jackson': 1, 'princ': 1, 'elton': 1, 'john': 1, 'zeppelin': 1, 'bee': 1, 'gee': 1, 'madonna': 1, 'queen': 1, 'marley': 1, 'black': 2, 'sabbath': 1, 'metallica': 1, 'smith': 1, 'bon': 1, 'jovi': 1, 'duran': 2, 'depech': 1, 'mode': 1, 'david': 1, 'bowi': 1, 'nirvana': 1, 'notori': 1, 'b.i.g.': 1, 'pink': 1, 'floyd': 1, 'elvi': 1, 'presley': 1, 'stone': 1, 'tupac': 1, 'shakur': 1, 'frank': 1, 'sinatra': 1, 'barbra': 1, 'streisand': 1, 'stevi': 1, 'wonder': 1, 'loui': 1, 'armstrong': 1, 'mile': 1, 'davi': 1, 'duke': 1, 'ellington': 1, 'ella': 1, 'fitzgerald': 1, 'charli': 1, 'parker': 1, 'robert': 1, 'johnson': 1, 'amr': 1, 'diab': 1, 'fairuz': 1, 'abdel': 1, 'halim': 1, 'hafez': 1, 'umm': 1, 'kulthum': 1, 'film': 9, 'medium': 1, 'movi': 1, 'theatr': 1, 'pittsburgh': 1, '1905': 1, '28': 1, 'center': 1, 'white': 2, 'technicolor': 1, 'color': 2, 'sound': 1, 'full-length': 3, 'featur': 2, 'jazz': 1, 'singer': 1, 'releas': 3, 'academi': 1, 'award': 1, '1929': 1, 'cel': 1, 'snow': 1, 'seven': 1, 'dwarf': 1, '1937': 1, 'computer-gener': 1, 'imageri': 1, '1980': 1, 'cgi-anim': 1, 'toy': 1, 'stori': 1, '1995': 1, 'games—du': 1, 'step': 1, 'forward': 1, 'period—ar': 1, 'entertain': 1, 'alongsid': 1, 'multipl': 1, 'field': 3, 'mathemat': 1, 'measur': 1, 'function': 1, 'analysi': 1, 'topolog': 1, 'abstract': 1, 'algebra': 1, 'probabl': 1, 'formal': 1, 'logic': 1, 'gödel': 1, 'incomplet': 1, 'theorem': 2, '32': 1, 'computationally-intens': 1, 'studi': 1, 'fractal': 1, '33': 1, 'proof': 1, '1976': 1, '34': 1, 'promin': 1, 'trait': 1, 'practic': 1, 'electron': 2, 'various': 1, 'call': 1, '39': 1, '41': 1, '42': 1, 'microelectron': 1, '43': 1, 'age': 2, '44': 1, 'silicon': 2, '45': 2, '46': 2, '47': 1, 'and/or': 1, 'third': 1, '48': 1}\n"
     ]
    }
   ],
   "source": [
    "print(freq_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'The 20t': 16.101888020833336, '[1]  Th': 19.875, 'Man-mad': 20.0, 'Additio': 17.88372093023256, 'Automob': 26.166666666666668, 'Great a': 25.90909090909091, 'The rep': 27.0, 'The Mar': 10.45945945945946, 'Through': 18.939393939393938, 'The dis': 16.28205128205128, 'It took': 14.877551020408163, '[6][7][': 19.416666666666668, 'Penicil': 10.818181818181818, '[9] Mac': 19.303030303030305, 'Trade i': 17.482758620689655, 'Until t': 16.289473684210527, '[10]\\nTh': 28.666666666666668, '[11] Th': 27.65, 'It was ': 22.958333333333332, 'Unlike ': 27.68421052631579, 'The cen': 29.067857142857143, 'Nationa': 18.71875, 'Terms l': 28.59090909090909, 'Scienti': 23.25739644970414, 'Horses ': 20.56, 'These d': 18.214285714285715, 'Humans ': 47.8, 'Mass me': 19.70967741935484, 'Advance': 11.9, 'Rapid t': 30.157894736842106, 'World W': 26.12, 'However': 52.6, 'For the': 26.56, 'The las': 24.136363636363637, '[12]\\nTh': 20.82051282051282, 'Technol': 24.0, 'After m': 16.238095238095237, 'In addi': 19.03846153846154, 'The Aus': 20.083333333333332, 'The Rus': 14.529411764705882, 'The vic': 36.25, 'At the ': 38.275510204081634, 'Fascism': 15.875, 'Meanwhi': 31.25, \"Japan's\": 25.88, 'After s': 19.928571428571427, 'After t': 17.312468019785094, 'In tota': 46.0, 'East Ge': 14.307692307692308, 'Western': 23.14814814814815, 'With th': 35.72222222222222, 'Allies ': 21.25925925925926, 'They fo': 10.916666666666666, 'The per': 19.21875, 'Mutuall': 22.181818181818183, 'Unable ': 19.59375, 'The tec': 18.964285714285715, 'In the ': 26.320105820105816, \"The US'\": 19.870967741935484, 'Britain': 21.22222222222222, 'Followi': 28.26086956521739, 'It enac': 28.2, 'Peaceke': 18.464285714285715, 'Europe ': 22.257575757575758, 'Due to ': 25.1, 'Inventi': 15.952380952380953, 'Althoug': 18.464285714285715, 'During ': 62.42857142857143, 'By the ': 31.58823529411765, '[15] At': 16.842105263157894, 'Communi': 33.266666666666666, 'The peo': 26.541666666666668, 'China, ': 27.045454545454547, 'The wor': 36.120448179271705, 'Since t': 38.61538461538461, 'The inf': 30.42105263157895, 'Terrori': 39.666666666666664, 'Disease': 26.571428571428573, 'New vir': 14.545454545454545, 'Malaria': 18.625, 'Million': 42.27272727272727, 'The vir': 18.0, 'Based o': 17.193548387096776, '[16] On': 19.4, '[17] Th': 29.157894736842106, 'World p': 15.470588235294118, '[18][19': 15.473684210526315, 'This in': 36.13333333333333, 'The dea': 16.823529411764707, '[citati': 20.15625, '[20] Ac': 20.75862068965517, 'Most li': 14.866666666666667, '\"[21] I': 22.17241379310345, '[22]\\nTh': 28.047619047619047, 'Prior t': 34.93333333333333, 'Many ne': 20.22222222222222, 'Popular': 4.977611940298507, 'Film as': 22.333333333333332, 'The fir': 14.5, '[28] Ho': 16.6, 'While t': 31.470588235294116, 'Sound f': 26.36842105263158, 'The Aca': 20.09090909090909, 'Animati': 21.214285714285715, 'Compute': 20.40740740740741, 'Video g': 10.0, 'Multipl': 20.0, 'The dev': 14.23076923076923, 'Later i': 41.583333333333336, '[32] Ot': 11.043478260869565, '[34]\\nOn': 18.8125, 'Organiz': 24.47826086956522, 'A techn': 14.311111111111112, '[48]': 16.2}\n"
     ]
    }
   ],
   "source": [
    "print(sent_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.316197406733412\n"
     ]
    }
   ],
   "source": [
    "print(avg_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'rouge-1': {'f': 0.15366772835813847, 'p': 1.0, 'r': 0.08322864321608041}, 'rouge-2': {'f': 0.14737452716112168, 'p': 0.9621212121212122, 'r': 0.07979893182532202}, 'rouge-l': {'f': 0.21442246139134, 'p': 1.0, 'r': 0.12008577555396711}}]\n"
     ]
    }
   ],
   "source": [
    "from rouge import Rouge \n",
    "rouge = Rouge()\n",
    "scores = rouge.get_scores(summary_results, article_content)\n",
    "print(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
